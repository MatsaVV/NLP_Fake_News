{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from textblob import TextBlob\n",
    "from joblib import Parallel, delayed\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mathieu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/mathieu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/mathieu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mathieu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mathieu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = pd.read_csv('../data/Fake.csv')\n",
    "true_data = pd.read_csv('../data/True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21417 entries, 0 to 21416\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    21417 non-null  object\n",
      " 1   text     21417 non-null  object\n",
      " 2   subject  21417 non-null  object\n",
      " 3   date     21417 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 669.4+ KB\n"
     ]
    }
   ],
   "source": [
    "fake_data.info()\n",
    "true_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "date       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_fake = fake_data.isnull().sum()\n",
    "missing_true = true_data.isnull().sum()\n",
    "missing_fake\n",
    "missing_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data_cleaned = fake_data.drop_duplicates()\n",
    "true_data_cleaned = true_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23478"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake_data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21211"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion en datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90029/550571077.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_data_cleaned['date'] = pd.to_datetime(fake_data_cleaned['date'], errors='coerce')\n",
      "/tmp/ipykernel_90029/550571077.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  true_data_cleaned['date'] = pd.to_datetime(true_data_cleaned['date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "fake_data_cleaned['date'] = pd.to_datetime(fake_data_cleaned['date'], errors='coerce')\n",
    "true_data_cleaned['date'] = pd.to_datetime(true_data_cleaned['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajout colonne label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90029/1346034667.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fake_data_cleaned['label'] = 'Fake'\n",
      "/tmp/ipykernel_90029/1346034667.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  true_data_cleaned['label'] = 'True'\n"
     ]
    }
   ],
   "source": [
    "fake_data_cleaned['label'] = 'Fake'\n",
    "true_data_cleaned['label'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinaison des deux jeux de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.concat([fake_data_cleaned, true_data_cleaned], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>2017-12-30</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>2017-12-29</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44684</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44685</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44686</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44687</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44688</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "44684  'Fully committed' NATO backs new U.S. approach...   \n",
       "44685  LexisNexis withdrew two products from Chinese ...   \n",
       "44686  Minsk cultural hub becomes haven from authorities   \n",
       "44687  Vatican upbeat on possibility of Pope Francis ...   \n",
       "44688  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "44684  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "44685  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "44686  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "44687  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "44688  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "            date label  \n",
       "0     2017-12-31  Fake  \n",
       "1     2017-12-31  Fake  \n",
       "2     2017-12-30  Fake  \n",
       "3     2017-12-29  Fake  \n",
       "4     2017-12-25  Fake  \n",
       "...          ...   ...  \n",
       "44684 2017-08-22  True  \n",
       "44685 2017-08-22  True  \n",
       "44686 2017-08-22  True  \n",
       "44687 2017-08-22  True  \n",
       "44688 2017-08-22  True  \n",
       "\n",
       "[44689 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('../data/combined_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer les caractères spéciaux, ponctuations, et guillemets simples et doubles\n",
    "    text = re.sub(r'[\"\\'“”‘’]', \" \", text)\n",
    "    text = re.sub(rf\"[{string.punctuation}\\\"']\", \" \", text)\n",
    "    # Tokenizer le texte\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Ajouter \"u\" aux stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('u')\n",
    "    # Supprimer les stop words et lemmatiser\n",
    "    filtered_text = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "        for word in word_tokens if word not in stop_words\n",
    "    ]\n",
    "    return filtered_text  # Retourner une liste de mots lemmatisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['title_normalized'] = combined_data['title'].apply(normalize_text)\n",
    "combined_data['text_normalized'] = combined_data['text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_normalized</th>\n",
       "      <th>text_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[donald, trump, sends, embarrass, new, year, e...</td>\n",
       "      <td>[donald, trump, wish, american, happy, new, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[drunk, bragging, trump, staffer, start, russi...</td>\n",
       "      <td>[house, intelligence, committee, chairman, dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sheriff, david, clarke, becomes, internet, jo...</td>\n",
       "      <td>[friday, reveal, former, milwaukee, sheriff, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[trump, obsess, even, obama, name, cod, websit...</td>\n",
       "      <td>[christmas, day, donald, trump, announce, woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pope, francis, call, donald, trump, christmas...</td>\n",
       "      <td>[pope, francis, use, annual, christmas, day, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44684</th>\n",
       "      <td>[fully, commit, nato, back, new, approach, afg...</td>\n",
       "      <td>[brussels, reuters, nato, ally, tuesday, welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44685</th>\n",
       "      <td>[lexisnexis, withdrew, two, product, chinese, ...</td>\n",
       "      <td>[london, reuters, lexisnexis, provider, legal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44686</th>\n",
       "      <td>[minsk, cultural, hub, becomes, authority]</td>\n",
       "      <td>[minsk, reuters, shadow, disused, soviet, era,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44687</th>\n",
       "      <td>[vatican, upbeat, possibility, pope, francis, ...</td>\n",
       "      <td>[moscow, reuters, vatican, secretary, state, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44688</th>\n",
       "      <td>[indonesia, buy, 1, 14, billion, worth, russia...</td>\n",
       "      <td>[jakarta, reuters, indonesia, buy, 11, sukhoi,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title_normalized  \\\n",
       "0      [donald, trump, sends, embarrass, new, year, e...   \n",
       "1      [drunk, bragging, trump, staffer, start, russi...   \n",
       "2      [sheriff, david, clarke, becomes, internet, jo...   \n",
       "3      [trump, obsess, even, obama, name, cod, websit...   \n",
       "4      [pope, francis, call, donald, trump, christmas...   \n",
       "...                                                  ...   \n",
       "44684  [fully, commit, nato, back, new, approach, afg...   \n",
       "44685  [lexisnexis, withdrew, two, product, chinese, ...   \n",
       "44686         [minsk, cultural, hub, becomes, authority]   \n",
       "44687  [vatican, upbeat, possibility, pope, francis, ...   \n",
       "44688  [indonesia, buy, 1, 14, billion, worth, russia...   \n",
       "\n",
       "                                         text_normalized  \n",
       "0      [donald, trump, wish, american, happy, new, ye...  \n",
       "1      [house, intelligence, committee, chairman, dev...  \n",
       "2      [friday, reveal, former, milwaukee, sheriff, d...  \n",
       "3      [christmas, day, donald, trump, announce, woul...  \n",
       "4      [pope, francis, use, annual, christmas, day, m...  \n",
       "...                                                  ...  \n",
       "44684  [brussels, reuters, nato, ally, tuesday, welco...  \n",
       "44685  [london, reuters, lexisnexis, provider, legal,...  \n",
       "44686  [minsk, reuters, shadow, disused, soviet, era,...  \n",
       "44687  [moscow, reuters, vatican, secretary, state, c...  \n",
       "44688  [jakarta, reuters, indonesia, buy, 11, sukhoi,...  \n",
       "\n",
       "[44689 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[['title_normalized', 'text_normalized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fréquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_title = FreqDist()\n",
    "fdist_text = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for words in combined_data['title_normalized']:\n",
    "    fdist_title.update(words)\n",
    "\n",
    "# Mettre à jour les fréquences avec les mots de chaque texte normalisé\n",
    "for words in combined_data['text_normalized']:\n",
    "    fdist_text.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 mots fréquents dans les titres :\n",
      "[('trump', 14679), ('video', 8569), ('say', 4276), ('obama', 3179), ('hillary', 2278), ('house', 1994), ('watch', 1983), ('republican', 1834), ('clinton', 1812), ('new', 1779)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 mots fréquents dans les titres :\")\n",
    "print(fdist_title.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 mots fréquents dans les textes :\n",
      "[('say', 168226), ('trump', 133647), ('state', 62931), ('president', 56728), ('would', 54810), ('people', 41809), ('year', 41429), ('republican', 39592), ('make', 39054), ('one', 38985)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 mots fréquents dans les textes :\")\n",
    "print(fdist_text.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colonnes pour les dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date    year  month  day_of_week  day_of_month  week_of_year\n",
      "0 2017-12-31  2017.0   12.0          6.0          31.0            52\n",
      "1 2017-12-31  2017.0   12.0          6.0          31.0            52\n",
      "2 2017-12-30  2017.0   12.0          5.0          30.0            52\n",
      "3 2017-12-29  2017.0   12.0          4.0          29.0            52\n",
      "4 2017-12-25  2017.0   12.0          0.0          25.0            52\n"
     ]
    }
   ],
   "source": [
    "combined_data['year'] = combined_data['date'].dt.year\n",
    "\n",
    "# Extraction du mois\n",
    "combined_data['month'] = combined_data['date'].dt.month\n",
    "\n",
    "# Extraction du jour de la semaine (lundi = 0, dimanche = 6)\n",
    "combined_data['day_of_week'] = combined_data['date'].dt.dayofweek\n",
    "\n",
    "# Extraction du jour du mois\n",
    "combined_data['day_of_month'] = combined_data['date'].dt.day\n",
    "\n",
    "# Extraction de l'heure (si disponible)\n",
    "if combined_data['date'].dt.hour.isna().sum() == 0:  # Vérifier si l'heure est présente\n",
    "    combined_data['hour'] = combined_data['date'].dt.hour\n",
    "\n",
    "# Extraction de la semaine de l'année\n",
    "combined_data['week_of_year'] = combined_data['date'].dt.isocalendar().week\n",
    "\n",
    "# Afficher un aperçu des nouvelles colonnes\n",
    "print(combined_data[['date', 'year', 'month', 'day_of_week', 'day_of_month', 'week_of_year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ne fonctionne pas (trop long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    return str(blob.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 4\n",
    "combined_data['title_corrected'] = Parallel(n_jobs=num_cores)(delayed(correct_spelling)(text) for text in combined_data['title_normalized'])\n",
    "combined_data['text_corrected'] = Parallel(n_jobs=num_cores)(delayed(correct_spelling)(text) for text in combined_data['text_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data[['title_corrected', 'text_corrected']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.to_csv('../data/combined_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
